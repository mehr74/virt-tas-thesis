%!TEX root = ../Thesis.tex

\section{Objectives}
Virt-TAS is designed to fulfill the following goals:

\begin{itemize}
    \item \textbf{Low latency and high throughput} % write about TAS
    Modern datacenters host distributed data-intensive applications, that require 
    low latency and high throughput. Workloads of these applications consist of short-lived 
    flows with stringent low latency requirements, as well as long-lived flows with 
    high throughput needs. VirtTAS should provide the latency and throughput needs 
    of these applications, and it should enable cloud providers to maintain Service 
    Level Agreements (SLAs). 
    
    \item \textbf{Compatibility} % write about openflow
    The networking interfaces should be unmodified so that applications residing on 
    the VMs could benefit from VirtTAS without any modification.
    Moreover, to make VirtTAS programmable through existing state-of-the-art network controllers,
    it should support OpenFlow API.

    \item \textbf{Mobility} % write about not being coupled to a physical network
    VMs could be migrated to different locations, thus VirtTAS should allow VMs to continue 
    communicating over the network, despite their migration. VirtTAS should decouple applications 
    from physical networking infrastructures at their location.

    \item \textbf{High utilization}
    As VirtTAS shares fate with the hypervisor, resource conversation is highly critical. 
    VirtTAS should maximize resource utilization to maintain precious resources for the 
    main goal of the hypervisor, running user workloads.

    \item \textbf{Scalability} % using a shared tcp fast-path
    Scalability becomes a challenging problem when different tenants with different network 
    topologies and networking demands reside on the same hypervisor. VirtTAS should keep up 
    with the increasing number of flows. It should also support an increasing number of flow 
    updates from a controller.

    \item \textbf{Isolation} 
    Applications residing on one VM must be prevented to access packets from other VMs. 
    They should not be able to violate other tenants' SLAs. Thus, VirtTAS should provide 
    the same isolation as the current architecture.

\end{itemize}

\section{Challenges}

\section{Design principles}
To achieve the aforementioned goals, we benefit from a set of design principles 
to provide network virtualization features in a shared TCP fastpath.

\section{Virt-TAS Architecture Overview}

Similar to TAS, Virt-TAS devides the network stack into three different components:
(\emph{i}) A fast-path running on hypervisor, (\emph{ii}) a slow-path that has components
both on the hypervisor and the VMs, and (\emph{iii}) an application library, which 
runs entirely on the VM side. 



\begin{figure}
    \centering
    \includegraphics[scale=0.324]{../Figures/design.bigpicture.pdf}
    \caption{Virt-TAS big picture}
    \label{fig:overhead.throughput}
\end{figure}


% a paragraph about the overview of ovs integration
Figure X depicts the overview architecture of Virt-TAS. The packets are first received 
by the fast-path component from a physical NIC or through shared memory from applications 
running on VMs. The fast-path either has received instruction from OVS on how to handle 
the received packet based on its flow or it has not. In the first scenario, the fast-path 
applies the action provided by OVS. The actions ranges from dropping the packet to forwarding 
it to specific application connected to Virt-TAS or to the physical NIC. In the second 
scenario, where fast-path has no information about the flow of the received packet,  the 
packet is delivered to OVS through lock-free shared memory. OVS can then decide how to treat 
packets. On the other side, the application interface has the responsibility to initialize the 
connection to Virt-TAS and implement POSIX network socket API for applications.

% a paragraph about the overview of sdn controller
By integrating OVS, we can control and program the forwarding dataplane through 
OpenFlow protocol. OpenFlow enables network operators to add, remove, update entries,
and to monitor statistics on flow tables. OVS takes OpenFlow tables from an SDN
controller, matches the received packets to these flow tables, and applies 
all actions needed to be taken. The outcome is then cached by the fast-path component
of Virt-TAS. This significantly simplifies the fast-path, as it allows the fast-path to 
be agnostic to OpenFlow specifics. One the other side, from the perspective of an OpenFlow 
controller,  processing packets in Virt-TAS is an invisible implementation detail. In its 
perspective, all packets are matched against multiple flow tables, and the corresponding 
actions are applied among entries that satisfy all conditions and have the highest priority.

% a paragraph about the proxy connection


% a paragraph about rediction of library calls 
A userspace library redirects the POSIX socket API transparently, so that applications 
remain unmodified. Through this library the sockets calls are transmitted to the 
hypervisor instead of being transfered to the VMs network stack. Furthermore, 
By rewriting applications to use low level API of libTAS higher performance can be achieved.

\section{Slow Path}
\subsection{Virtual switching}
\subsection{Proxy server on hypervisor}
\subsection{Proxy guest on VMs}
\subsection{Application Interface}

\section{Fast Path}
\subsection{Connection setup}
\subsection{OpenFlow action caching}

\section{Application library}