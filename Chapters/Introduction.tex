%!TEX root = ../Thesis.tex

%---------------------------------------------------------------------------------------------%
% a paragraph about virtualization, that is widely used
Virtualization has become increasingly popular over the last two decades. Enabling fast 
provisioning, increasing the availability time, and reducing the maintenance costs
for users are a few reasons of the gained popularity. Moreover, by multiplexing 
multiple virtual machines (VMs) on a same physical server less enegry gets consumed, 
which makes virtualization enviromental friendly and even more popular thesedays.

%---------------------------------------------------------------------------------------------%
% a paragraph about networking in virtualized environment
Public cloud providers such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud
Platform (GCP) are expanding the range of workloads which can be spilled over virtualized environments. 
These  vendors, in addition to providing high number of VMs with increased performance, have made 
it possible for public tenants to migrate their home workloads without changing the 
on-premises networking configurations. Furthermore, the users of these environments
benefit from supplied address spaces, security groups and ACLs, scalable load balancers, 
bandwidth metering, QoS, and more \cite{firestone2017vfp}.


%---------------------------------------------------------------------------------------------%
% about new stacks for processing tcp
The workloads applied by cloud tenants are bursty, which require high throughput and low latecy 
network access. It was this requirement, which motivated cloud providers to increase their networking 
speeds by more than 40x and more in a span of only a few years \cite{firestone2018azure}. While 
networking speed is growing fast, the CPU improvement has become slower, and has experienced the 
end of Moore's law \cite{esmaeilzadeh2011dark}. Hence efficient packet processing is becoming 
more and more important. 

Due to poor cache allocation, costly context switches, and resources sharing across different 
cores, Linux TCP packet processing is considered as inefficient for modern data center 
networking \cite{kaufmann2019tas, shashidhara2022flextoe}. Different techniques tried 
to address the issue by reducing the overheads and improving the conventional TCP stack.
These techniques include bypassing kernel to enable direct NIC access from user-space
\cite{belay2014ix, jeong2014mtcp, prekas2017zygos}
using receive side scaling (RSS) to carefuly steering and processing packets on multi-cores 
architecture \cite{marty2019snap, kaufmann2019tas}, and offloading packet processing to 
NICs with computation capabilities 
\cite{arashloo2020enabling, lin2020panic, firestone2018azure, shashidhara2022flextoe}. 


%---------------------------------------------------------------------------------------------%
% define problem 
% why modern tcp stacks are not applicable by VMs
Although these techniques have imporved the performance of end-host packet processing, they 
suffer from two fundamental implications when it comes to providing high throughput and 
low latency network access for applications residing on VMs.


% operators hesitate to deploy new stacks 
\textbf{Implication I.} Majority of these techniques are proposed only for applications running
on bare metal servers, and they do not provide the rich virtualization features required in 
multi-tentant datacenters. Hence, cloud providers hesitate to deploy the techniques for
their tenants and applications still use the conventional TCP packet processing.

% users hesitate to use new stacks 
\textbf{Implication II.} Cloud tenants are only concerned with performance of their applications,
and they do not want the maintenance of the network stack to fallout on their shoulders. Deploying 
new packet processing stacks requires extensive testing and comes with the risk of 
misconfigurations. Therefore, cloud tenants hesitate to deploy the new techniques in trade with
high throughput and low latency.

%---------------------------------------------------------------------------------------------%
% goal
% we asked ourself whether it is possible to provide
% TODO extend 
\textbf{Goal} In this thesis, we ask the following question: \emph{How can we provide virtualization 
features on top of the modern stacks?} In response, we enrich Virt-TAS with virtualization features.
Virt-TAS is a TCP acceleration service (TAS) for virtualized environments targeted at applications 
that require low latency and high throughput. It runs as a service alongside the host and provides 
a fast-path for common send and receive operations, through which it reduces the virtualization 
overheads incurred by the hypervisor and guest operating system \cite{florian2021virttas}. 

%---------------------------------------------------------------------------------------------%
% briefly explain how VirtTAS works
% TODO extend
To avoid reinventing the wheel, instead of implementing a virtual switch from scratch, we
complement Virt-TAS by integrating Open vSwitch (OVS) \cite{pfaff2015design}. OVS is a virtual
switch that is deployed in many data center networks. It is also a component of VMware's NSX 
product, used by thousands of customers \cite{tu2021revisiting}.


NetKernel, is another system that provides network stack as a service in the cloud
\cite{niu2021netkernel, niu2017network}.
Like Virt-TAS, Netkernel decouples the network stack from the guest OS, 
(\emph{i}) to simplify deployment and maintenance for tenants, and (\emph{ii}) to increase  
efficiency of TCP packet processing for cloud tenants. Netkernel's 
goal is to show the feasibility of decoupling network stack from VMs, whereas Virt-TAS
focuses on providing efficient packet processing while providing network virtualization 
features on top of decoupling network stack.

%---------------------------------------------------------------------------------------------%
% what are the main contributions of this thesis
% TODO extend
This thesis makes the following contributions.
\begin{itemize}
    \item 
    We have made an extension to the memory layout of Virt-TAS to incorporate the 
    concept of \emph{groups}. Inspired by Amazon EC2's target groups \cite{ec2target}
    , a group in Virt-TAS is a set of virtual machines (VMs) that trust each other and 
    can share memory. In this design, each VM in Virt-TAS is assigned to a group, 
    and a group can be assigned to multiple VMs. This extension improves the flexibility 
    and scalability of the Virt-TAS system and allows for more efficient memory sharing
     among VMs.

    \item We integrate OVS to a TCP Acceleration Service (TAS) to leverage the 
    network programmability features provided by the OpenFlow interface. In the new design,
    each VM is connected to OVS through a shared memory, managed by TAS.
    TAS, on the other hand, facilitates communication between the NIC and OVS, as well as 
    between applications on VMs and the hypervisor, through lock-free queues on shared memories.
    The design enables us to take advantage 
    of OVS's network programmability capabilities and benefit from improved network performance 
    through streamlininig TCP packet processing enabled by TAS.

    \item As a further optimization, we leverage fast-path processing in our solution. 
    Virt-TAS processes the first packet of each connection in Open vSwitch (OVS) and 
    stores the connection information for the flow in the cache of TAS for bridge-only 
    connections. The subsequent packets utilize the fast-path processing. 
    This optimization results in improved performance and reduced overhead in 
    the processing of network flows. 

\end{itemize}

%---------------------------------------------------------------------------------------------%
% structure of next sections
% TODO extend
The remainder of this thesis is structured as follows: Chapter 2
provides a comprehensive background on network virtualization,
including a review of the state-of-the-art solutions. Chapter 3
delves into the design of Virt-TAS, as well as outlining the 
design requirements. Chapter 4 explains the implementation of 
Virt-TAS. In chapter 5, we evaluate Virt-TAS based on three 
predefined requirements. Finally, chapter 6 provides a 
conclusion of our work, summarizing our findings
and contributions to the field of network virtualization.

